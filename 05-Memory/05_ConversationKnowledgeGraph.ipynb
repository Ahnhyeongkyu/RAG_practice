{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationKGMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting networkx\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/1.7 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 26.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 21.7 MB/s eta 0:00:00\n",
      "Installing collected packages: networkx\n",
      "Successfully installed networkx-3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "memory = ConversationKGMemory(llm=llm, return_messages=True)\n",
    "memory.save_context(\n",
    "    {\"input\": \"이쪽은 Pangyo 에 거주중인 김셜리씨 입니다.\"},\n",
    "    {\"output\": \"김셜리씨는 누구시죠?\"},\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"김셜리씨는 우리 회사의 신입 디자이너입니다.\"},\n",
    "    {\"output\": \"만나서 반갑습니다.\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Pangyo: Pangyo is in Kim Shelley.'),\n",
       "  SystemMessage(content='On 김셜리씨: 김셜리씨 is a 신입 디자이너. 김셜리씨 is in 우리 회사.')]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"김셜리씨는 누구입니까?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain 에 메모리 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\UserK\\miniforge3\\envs\\rag\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. \n",
    "The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "conversation_with_kg = ConversationChain(\n",
    "    llm=llm, prompt=prompt, memory=ConversationKGMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Teddy! It's nice to meet you. Shirley must be excited to be starting a new job as a designer at your company. I hope she's settling in well and getting to know everyone. If you need any tips on how to make her feel welcome or help her adjust to the new role, feel free to ask me!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.predict(\n",
    "    input=\"My name is Teddy. Shirley is a coworker of mine, and she's a new designer at our company.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'On Shirley: Shirley is a coworker. Shirley is a new designer. Shirley is at company.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shirley 에 대한 질문\n",
    "conversation_with_kg.memory.load_memory_variables({\"input\": \"who is Shirley?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
