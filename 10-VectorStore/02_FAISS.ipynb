{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (0.2.9)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (0.1.17)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-community) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.9 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-community) (0.2.10)\n",
      "Collecting langchain-core<0.3.0,>=0.2.23 (from langchain-community)\n",
      "  Downloading langchain_core-0.2.24-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-community) (0.1.93)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.32.0 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-openai) (1.37.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain<0.3.0,>=0.2.9->langchain-community) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain<0.3.0,>=0.2.9->langchain-community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.23->langchain-community) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.9->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.9->langchain-community) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.32.0->langchain-openai) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\anhye\\anaconda3\\envs\\rag\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Downloading langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.3 MB 991.0 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.5/2.3 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.9/2.3 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.3 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.3 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.3 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 8.1 MB/s eta 0:00:00\n",
      "Downloading langchain_openai-0.1.19-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.1/47.1 kB ? eta 0:00:00\n",
      "Downloading langchain_core-0.2.24-py3-none-any.whl (377 kB)\n",
      "   ---------------------------------------- 0.0/377.3 kB ? eta -:--:--\n",
      "   --------------------------------------  368.6/377.3 kB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 377.3/377.3 kB 7.8 MB/s eta 0:00:00\n",
      "Installing collected packages: langchain-core, langchain-openai, langchain-community\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.22\n",
      "    Uninstalling langchain-core-0.2.22:\n",
      "      Successfully uninstalled langchain-core-0.2.22\n",
      "  Attempting uninstall: langchain-openai\n",
      "    Found existing installation: langchain-openai 0.1.17\n",
      "    Uninstalling langchain-openai-0.1.17:\n",
      "      Successfully uninstalled langchain-openai-0.1.17\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.2.9\n",
      "    Uninstalling langchain-community-0.2.9:\n",
      "      Successfully uninstalled langchain-community-0.2.9\n",
      "Successfully installed langchain-community-0.2.10 langchain-core-0.2.24 langchain-openai-0.1.19\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U langchain-community faiss-cpu langchain-openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서는 문서를 벡터 저장소에 수집합니다.\n",
    "\n",
    "- TextLoader를 사용하여 텍스트 데이터를 로드합니다.\n",
    "- CharacterTextSplitter를 사용하여 로드된 문서를 300 자 단위로 분할하고, 분할된 문서 간에 중복되는 내용이 없도록 설정합니다.\n",
    "- OpenAIEmbeddings를 사용하여 문서 임베딩을 생성합니다.\n",
    "- FAISS 벡터 저장소를 초기화하고, 분할된 문서와 임베딩을 사용하여 벡터 인덱스를 구축합니다.\n",
    "\n",
    "참고\n",
    "\n",
    "- AVX2: 고도의 병렬 처리가 가능한 연산을 사용하는 벡터화 가능 알고리즘의 경우 AVX2 를 사용하면 CPU 성능이 향상되어 지연 시간이 줄어들며 처리량이 향상됩니다.\n",
    "- 필요한 경우 os.environ['FAISS_NO_AVX2'] = '1' 코드 라인의 주석을 해제하여 FAISS에서 AVX2 최적화를 사용하지 않도록 설정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# FAISS에서 AVX2 최적화를 사용하지 않으려면 다음 줄의 주석을 해제하세요.\n",
    "# import os\n",
    "#\n",
    "# os.environ['FAISS_NO_AVX2'] = '1'\n",
    "\n",
    "# TextLoader를 사용하여 텍스트 파일을 로드합니다.\n",
    "loader = TextLoader(\"./data/appendix-keywords.txt\")\n",
    "\n",
    "# 로드된 문서를 가져옵니다.\n",
    "documents = loader.load()\n",
    "\n",
    "# CharacterTextSplitter를 사용하여 문서를 분할합니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 분할된 문서를 가져옵니다.\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# FAISS를 사용하여 문서와 임베딩으로부터 데이터베이스를 생성합니다.\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "source": [
    "# data/appendix-keywords.txt 파일 내용을 읽어서 file 변수에 저장합니다.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n",
    "\n",
    "# file 변수에 저장된 내용을 출력합니다.\n",
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리(query) 변수에 저장된 질문과 유사한 문서를 데이터베이스에서 검색합니다.\n",
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "docs = db.similarity_search(query)  # 질문과 유사한 문서를 데이터베이스에서 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n"
     ]
    }
   ],
   "source": [
    "# docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터베이스를 검색기로 사용하기 위해 retriever 변수에 할당합니다.\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 질의를 사용하여 관련 문서를 검색합니다.\n",
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n"
     ]
    }
   ],
   "source": [
    "# docs 리스트의 첫 번째 요소의 page_content 속성을 출력합니다.\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 점수에 기반한 유사도 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Content]\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "[Score]\n",
      "0.29650295\n"
     ]
    }
   ],
   "source": [
    "# 쿼리와 유사한 문서를 검색하고 유사도 점수와 함께 반환합니다.\n",
    "docs_and_scores = db.similarity_search_with_score(query)\n",
    "content, score = docs_and_scores[0]  # 문서와 점수 리스트에서 첫 번째 요소를 선택합니다\n",
    "print(\"[Content]\")\n",
    "print(content.page_content)  # 선택된 문서의 page_content 속성을 출력합니다\n",
    "print(\"\\n[Score]\")\n",
    "print(score)  # 선택된 문서의 점수를 출력합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/appendix-keywords.txt'}, page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 질의를 임베딩 벡터로 변환합니다.\n",
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "embedding_vector = embeddings.embed_query(query)\n",
    "# 임베딩 벡터를 사용하여 유사도 검색을 수행하고, 문서와 점수를 반환합니다.\n",
    "docs_and_scores = db.similarity_search_by_vector(embedding_vector)\n",
    "docs_and_scores[0]  # 문서와 점수 리스트에서 첫 번째 요소를 선택합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬에 \"MY_FIRST_DB_INDEX\"라는 이름으로 데이터베이스를 저장합니다.\n",
    "DB_INDEX = \"MY_FIRST_DB_INDEX\"\n",
    "db.save_local(DB_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/appendix-keywords.txt'}, page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 로컬에 저장된 데이터베이스를 불러와 new_db 변수에 할당합니다.\n",
    "new_db = FAISS.load_local(DB_INDEX, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "\n",
    "# new_db에서 query와 유사한 문서를 검색하여 docs 변수에 할당합니다.\n",
    "docs = new_db.similarity_search(query)\n",
    "\n",
    "# 문서 리스트의 첫 번째 문서를 가져옵니다.\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 바이트 형태로 직렬화/역직렬화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anHye\\anaconda3\\envs\\rag\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\anHye\\anaconda3\\envs\\rag\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\anHye\\anaconda3\\envs\\rag\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\anHye\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# TextLoader를 사용하여 텍스트 파일을 로드합니다.\n",
    "loader = TextLoader(\"./data/appendix-keywords.txt\")\n",
    "\n",
    "# 로드된 문서를 가져옵니다.\n",
    "documents = loader.load()\n",
    "\n",
    "# CharacterTextSplitter를 사용하여 문서를 분할합니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 분할된 문서를 가져옵니다.\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# HuggingFaceEmbeddings 사용하여 임베딩을 생성합니다.\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# FAISS를 사용하여 문서와 임베딩으로부터 데이터베이스를 생성합니다.\n",
    "db = FAISS.from_documents(docs, hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 인덱스를 직렬화합니다.\n",
    "serialized_db_index = db.serialize_to_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'60.42 KB'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# 사이즈 측정을 위한 함수를 정의합니다.\n",
    "\n",
    "\n",
    "def get_size(path):\n",
    "    size = sys.getsizeof(path)\n",
    "    if size < 1024:\n",
    "        return f\"{size} bytes\"\n",
    "    elif size < pow(1024, 2):\n",
    "        return f\"{round(size/1024, 2)} KB\"\n",
    "    elif size < pow(1024, 3):\n",
    "        return f\"{round(size/(pow(1024,2)), 2)} MB\"\n",
    "    elif size < pow(1024, 4):\n",
    "        return f\"{round(size/(pow(1024,3)), 2)} GB\"\n",
    "\n",
    "\n",
    "# 직렬화된 FAISS 인덱스의 사이즈를 출력합니다.\n",
    "get_size(serialized_db_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직렬화된 인덱스를 로드합니다.\n",
    "deserialized_db = FAISS.deserialize_from_bytes(\n",
    "    embeddings=hf_embeddings,  # 직렬화 할 때의 임베딩과 동일하게 지정\n",
    "    serialized=serialized_db_index,  # 직렬화된 인덱스\n",
    "    allow_dangerous_deserialization=True  # 위험한 디직렬화를 허용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/appendix-keywords.txt'}, page_content='정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\\n예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\\n연관키워드: 데이터 교환, 웹 개발, API\\n\\nTransformer')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "\n",
    "# new_db에서 query와 유사한 문서를 검색하여 docs 변수에 할당합니다.\n",
    "docs = deserialized_db.similarity_search(query)\n",
    "\n",
    "# 문서 리스트의 첫 번째 문서를 가져옵니다.\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 병합(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# db1 생성\n",
    "db1 = FAISS.from_texts([\"LangChain!!\"], embeddings)\n",
    "# db2 생성\n",
    "db2 = FAISS.from_texts([\"좋아요^^\"], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3b8a020b-ba51-4235-94cf-4c34b3c73a1d': Document(page_content='LangChain!!')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db1 벡터 데이터베이스의 내부 문서 저장소 딕셔너리에 접근합니다.\n",
    "db1.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c613ab97-a9ac-45de-ab64-26416c60e52f': Document(page_content='좋아요^^')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db2 문서 저장소의 내부 딕셔너리에 접근합니다.\n",
    "db2.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1.merge_from(db2)  # db1에 db2를 병합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'3b8a020b-ba51-4235-94cf-4c34b3c73a1d': Document(page_content='LangChain!!'),\n",
       " 'c613ab97-a9ac-45de-ab64-26416c60e52f': Document(page_content='좋아요^^')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db1 문서 저장소의 내부 딕셔너리에 접근합니다.\n",
    "db1.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: foo, Metadata: {'page': 1}, Score: 0.0\n",
      "Content: foo, Metadata: {'page': 2}, Score: 0.0\n",
      "Content: foo, Metadata: {'page': 3}, Score: 0.0\n",
      "Content: foo, Metadata: {'page': 4}, Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "list_of_documents = [\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 1을 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=1)),\n",
    "    # 페이지 내용이 \"bar\"이고 메타데이터로 페이지 번호 1을 가진 문서\n",
    "    Document(page_content=\"bar\", metadata=dict(page=1)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 2를 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=2)),\n",
    "    # 페이지 내용이 \"barbar\"이고 메타데이터로 페이지 번호 2를 가진 문서\n",
    "    Document(page_content=\"barbar\", metadata=dict(page=2)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 3을 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=3)),\n",
    "    # 페이지 내용이 \"bar burr\"이고 메타데이터로 페이지 번호 3을 가진 문서\n",
    "    Document(page_content=\"bar burr\", metadata=dict(page=3)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 4를 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=4)),\n",
    "    # 페이지 내용이 \"bar bruh\"이고 메타데이터로 페이지 번호 4를 가진 문서\n",
    "    Document(page_content=\"bar bruh\", metadata=dict(page=4)),\n",
    "]\n",
    "# 문서 리스트와 임베딩을 사용하여 FAISS 데이터베이스 생성\n",
    "db = FAISS.from_documents(list_of_documents, embeddings)\n",
    "\n",
    "# \"foo\"와 유사한 문서를 검색하고 점수와 함께 결과 반환\n",
    "results_with_scores = db.similarity_search_with_score(\"foo\")\n",
    "\n",
    "for doc, score in results_with_scores:  # 검색 결과를 반복하면서\n",
    "    # 각 문서의 내용, 메타데이터, 점수를 출력\n",
    "    print(\n",
    "        f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Content] foo, [metadata] {'page': 1}, [Score] 0.0\n",
      "[Content] bar, [metadata] {'page': 1}, [Score] 0.3147187829017639\n"
     ]
    }
   ],
   "source": [
    "# 방법1) 유사도 검색을 수행하고 필터를 적용하여 결과와 점수를 반환합니다.\n",
    "results_with_scores = db.similarity_search_with_score(\"foo\", filter=dict(page=1))\n",
    "# 방법2) 혹은 callable 을 사용하여 필터링 하는 경우\n",
    "# results_with_scores = db.similarity_search_with_score(\"foo\", filter=lambda d: d[\"page\"] == 1)\n",
    "\n",
    "for doc, score in results_with_scores:  # 결과와 점수를 반복합니다.\n",
    "    # 각 문서의 내용, 메타데이터, 점수를 출력합니다.\n",
    "    print(f\"[Content] {doc.page_content}, [metadata] {doc.metadata}, [Score] {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Marginal Relevance (MMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- db.max_marginal_relevance_search() 메서드를 사용하여 \"foo\"라는 검색어로 문서를 검색합니다.\n",
    "- 이때 filter 매개변수를 사용하여 page 메타데이터 값이 1인 문서만 검색합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Content] foo, [metadata] {'page': 1}\n",
      "[Content] bar, [metadata] {'page': 1}\n"
     ]
    }
   ],
   "source": [
    "# \"foo\"를 검색어로 사용하여 최대 한계 관련성 검색을 수행하고, 메타데이터의 'page' 필드가 1인 문서만 필터링합니다.\n",
    "results = db.max_marginal_relevance_search(\"foo\", filter=dict(page=1))\n",
    "\n",
    "for doc in results:\n",
    "    # 각 문서의 내용과 메타데이터를 출력합니다.\n",
    "    print(f\"[Content] {doc.page_content}, [metadata] {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Content] foo, [metadata] {'page': 1}\n"
     ]
    }
   ],
   "source": [
    "results = db.similarity_search(\n",
    "    \"foo\",  # 검색 쿼리\n",
    "    # 메타데이터의 'page' 필드가 1인 문서만 필터링\n",
    "    filter=dict(page=1),\n",
    "    k=1,  # 가장 유사한 1개의 문서를 반환\n",
    "    fetch_k=4,\n",
    ")  # 4개의 문서까지 검색\n",
    "\n",
    "for doc in results:\n",
    "    # 각 문서의 내용과 메타데이터를 출력합니다.\n",
    "    print(f\"[Content] {doc.page_content}, [metadata] {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f78c43b0-9fed-497c-80ff-fc1ea4bbe0c3'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index_to_docstore_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스 0에 해당하는 문서 저장소 ID를 사용하여 데이터베이스에서 문서를 삭제합니다.\n",
    "db.delete([db.index_to_docstore_id[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
