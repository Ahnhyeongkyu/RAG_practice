{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS 비동기(Asynchronous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "faiss-gpu 또는 faiss-cpu 라이브러리를 설치합니다.\n",
    "\n",
    "- CUDA 7.5 이상을 지원하는 GPU가 있는 경우 faiss-gpu를 설치합니다.\n",
    "- GPU가 없거나 CPU에서 실행하려는 경우 faiss-cpu를 설치합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade --quiet faiss-cpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TextLoader를 사용하여 텍스트 데이터를 로드합니다.\n",
    "- CharacterTextSplitter를 사용하여 로드된 문서를 1000자 단위로 분할하고, 분할된 문서 간에 중복되는 내용이 없도록 설정합니다.\n",
    "- OpenAIEmbeddings를 사용하여 문서 임베딩을 생성합니다.\n",
    "- FAISS 벡터 저장소를 초기화하고, 분할된 문서와 임베딩을 사용하여 벡터 인덱스를 구축합니다.\n",
    "\n",
    "참고\n",
    "\n",
    "- AVX2: 고도의 병렬 처리가 가능한 연산을 사용하는 벡터화 가능 알고리즘의 경우 AVX2 를 사용하면 CPU 성능이 향상되어 지연 시간이 줄어들며 처리량이 향상됩니다.\n",
    "- 필요한 경우 os.environ['FAISS_NO_AVX2'] = '1' 코드 라인의 주석을 해제하여 FAISS에서 AVX2 최적화를 사용하지 않도록 설정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다.\n",
      "연관키워드: 토큰화, 자연어\n"
     ]
    }
   ],
   "source": [
    "# data/appendix-keywords.txt 파일 내용을 읽어서 file 변수에 저장합니다.\n",
    "with open(\"./data/appendix-keywords.txt\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n",
    "\n",
    "# file 변수에 저장된 내용을 출력합니다.\n",
    "print(file[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "# FAISS에서 AVX2 최적화를 사용하지 않으려면 다음 줄의 주석을 해제하세요.\n",
    "# import os\n",
    "#\n",
    "# os.environ['FAISS_NO_AVX2'] = '1'\n",
    "\n",
    "# TextLoader를 사용하여 텍스트 파일을 로드합니다.\n",
    "loader = TextLoader(\"./data/appendix-keywords.txt\")\n",
    "\n",
    "# 로드된 문서를 가져옵니다.\n",
    "documents = loader.load()\n",
    "\n",
    "# CharacterTextSplitter를 사용하여 문서를 분할합니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 분할된 문서를 가져옵니다.\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# FAISS를 사용하여 문서와 임베딩으로부터 데이터베이스를 생성합니다.\n",
    "db = await FAISS.afrom_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n"
     ]
    }
   ],
   "source": [
    "# 검색할 쿼리 설정\n",
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "\n",
    "# 쿼리와 유사한 문서 검색\n",
    "docs = await db.asimilarity_search(query)\n",
    "\n",
    "# 검색된 문서의 내용 출력\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 점수에 기반한 유사도 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(metadata={'source': './data/appendix-keywords.txt'}, page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken'),\n",
       " 0.29650295)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 쿼리와 유사한 문서를 검색하고 유사도 점수와 함께 반환합니다.\n",
    "docs_and_scores = await db.asimilarity_search_with_score(query)\n",
    "\n",
    "# 검색 결과 중 가장 유사도가 높은 문서와 점수를 가져옵니다.\n",
    "docs_and_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색할 쿼리 설정\n",
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "# 쿼리를 임베딩 벡터로 변환합니다.\n",
    "embedding_vector = await embeddings.aembed_query(query)\n",
    "# 임베딩 벡터를 사용하여 유사도 검색을 수행하고 문서와 점수를 반환합니다.\n",
    "docs_and_scores = await db.asimilarity_search_by_vector(embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬에 \"MY_ASYNC_DB_INDEX\"라는 이름으로 데이터베이스를 저장합니다.\n",
    "DB_INDEX = \"MY_ASYNC_DB_INDEX\"\n",
    "db.save_local(DB_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/appendix-keywords.txt'}, page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 로컬에 저장된 데이터베이스를 불러와 new_db 변수에 할당합니다.\n",
    "new_db = FAISS.load_local(DB_INDEX, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "\n",
    "# new_db에서 query와 유사한 문서를 비동기적으로 검색하여 docs 변수에 할당합니다.\n",
    "docs = await new_db.asimilarity_search(query)\n",
    "\n",
    "docs[0]  # 검색 결과 중 가장 유사한 문서를 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 바이트 형태로 직렬화/역직렬화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anHye\\anaconda3\\envs\\rag\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\anHye\\anaconda3\\envs\\rag\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# TextLoader를 사용하여 텍스트 파일을 로드합니다.\n",
    "loader = TextLoader(\"./data/appendix-keywords.txt\")\n",
    "\n",
    "# 로드된 문서를 가져옵니다.\n",
    "documents = loader.load()\n",
    "\n",
    "# CharacterTextSplitter를 사용하여 문서를 분할합니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 분할된 문서를 가져옵니다.\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# HuggingFaceEmbeddings 사용하여 임베딩을 생성합니다.\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# FAISS를 사용하여 문서와 임베딩으로부터 데이터베이스를 생성합니다.\n",
    "db = FAISS.from_documents(docs, hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 인덱스를 직렬화합니다.\n",
    "serialized_db_index = db.serialize_to_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'60.42 KB'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# 사이즈 측정을 위한 함수를 정의합니다.\n",
    "\n",
    "\n",
    "def get_size(path):\n",
    "    size = sys.getsizeof(path)\n",
    "    if size < 1024:\n",
    "        return f\"{size} bytes\"\n",
    "    elif size < pow(1024, 2):\n",
    "        return f\"{round(size/1024, 2)} KB\"\n",
    "    elif size < pow(1024, 3):\n",
    "        return f\"{round(size/(pow(1024,2)), 2)} MB\"\n",
    "    elif size < pow(1024, 4):\n",
    "        return f\"{round(size/(pow(1024,3)), 2)} GB\"\n",
    "\n",
    "\n",
    "# 직렬화된 FAISS 인덱스의 사이즈를 출력합니다.\n",
    "get_size(serialized_db_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직렬화된 인덱스를 로드합니다.\n",
    "deserialized_db = FAISS.deserialize_from_bytes(\n",
    "    embeddings=hf_embeddings,  # 직렬화 할 때의 임베딩과 동일하게 지정\n",
    "    serialized=serialized_db_index,  # 직렬화된 인덱스\n",
    "    allow_dangerous_deserialization=True  # 위험한 디직렬화를 허용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/appendix-keywords.txt'}, page_content='정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다.\\n예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다.\\n연관키워드: 데이터 교환, 웹 개발, API\\n\\nTransformer')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"임베딩(Embedding)이란 무엇인가요?\"\n",
    "\n",
    "# new_db에서 query와 유사한 문서를 검색하여 docs 변수에 할당합니다.\n",
    "docs = deserialized_db.similarity_search(query)\n",
    "\n",
    "# 문서 리스트의 첫 번째 문서를 가져옵니다.\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 병합(merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FAISS.afrom_texts 메서드를 사용하여 두 개의 FAISS 데이터베이스 db1과 db2를 비동기적으로 생성합니다.\n",
    "- db1은 [\"foo\"] 텍스트 리스트로부터 생성되며, embeddings를 사용하여 벡터화됩니다.\n",
    "- db2는 [\"bar\"] 텍스트 리스트로부터 생성되며, 동일한 embeddings를 사용하여 벡터화됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# db1 생성\n",
    "db1 = FAISS.from_texts([\"LangChain DB 1의 내용\"], embeddings)\n",
    "# db2 생성\n",
    "db2 = FAISS.from_texts([\"DB 2 의 내용\"], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e0b4e0ef-bea5-4ef0-83e2-ff27ef869d6a': Document(page_content='LangChain DB 1의 내용')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db1 벡터 데이터베이스의 내부 문서 저장소 딕셔너리에 접근합니다.\n",
    "db1.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1f6d1fe6-4d27-4253-9476-ffd6648ff0f2': Document(page_content='DB 2 의 내용')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db2 문서 저장소의 내부 딕셔너리에 접근합니다.\n",
    "db2.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1.merge_from(db2)  # db1에 db2를 병합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e0b4e0ef-bea5-4ef0-83e2-ff27ef869d6a': Document(page_content='LangChain DB 1의 내용'),\n",
       " '1f6d1fe6-4d27-4253-9476-ffd6648ff0f2': Document(page_content='DB 2 의 내용')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db1 문서 저장소의 내부 딕셔너리에 접근합니다.\n",
    "db1.docstore._dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "list_of_documents = [\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 1을 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=1)),\n",
    "    # 페이지 내용이 \"bar\"이고 메타데이터로 페이지 번호 1을 가진 문서\n",
    "    Document(page_content=\"bar\", metadata=dict(page=1)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 2를 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=2)),\n",
    "    # 페이지 내용이 \"barbar\"이고 메타데이터로 페이지 번호 2를 가진 문서\n",
    "    Document(page_content=\"barbar\", metadata=dict(page=2)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 3을 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=3)),\n",
    "    # 페이지 내용이 \"bar burr\"이고 메타데이터로 페이지 번호 3을 가진 문서\n",
    "    Document(page_content=\"bar burr\", metadata=dict(page=3)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 4를 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=4)),\n",
    "    # 페이지 내용이 \"bar bruh\"이고 메타데이터로 페이지 번호 4를 가진 문서\n",
    "    Document(page_content=\"bar bruh\", metadata=dict(page=4)),\n",
    "]\n",
    "# 문서 리스트와 임베딩을 사용하여 FAISS 데이터베이스 생성\n",
    "db = FAISS.from_documents(list_of_documents, embeddings)\n",
    "results_with_scores = db.asimilarity_search_with_score(\n",
    "    \"foo\"\n",
    ")  # \"foo\"와 유사한 문서를 검색하고 점수와 함께 결과 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: foo, Metadata: {'page': 1}, Score: 9.071771273738705e-06\n",
      "Content: bar, Metadata: {'page': 1}, Score: 0.3146589398384094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anHye\\AppData\\Local\\Temp\\ipykernel_26528\\3662698638.py:2: RuntimeWarning: coroutine 'FAISS.asimilarity_search_with_score' was never awaited\n",
      "  results_with_scores = await db.asimilarity_search_with_score(\"foo\", filter=dict(page=1))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# 유사도 검색을 수행하고 결과를 점수와 함께 가져옵니다. 필터로 page가 1인 문서만 검색합니다.\n",
    "results_with_scores = await db.asimilarity_search_with_score(\"foo\", filter=dict(page=1))\n",
    "for (\n",
    "    doc,\n",
    "    score,\n",
    ") in results_with_scores:  # 검색 결과를 반복하면서 각 문서와 점수를 가져옵니다.\n",
    "    # 문서의 내용, 메타데이터, 점수를 출력합니다.\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Marginal Relevance (MMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: foo, Metadata: {'page': 1}\n",
      "Content: bar, Metadata: {'page': 1}\n"
     ]
    }
   ],
   "source": [
    "# \"foo\"를 검색어로 사용하여 최대 한계 관련성 검색을 수행하고, 필터로 page가 1인 문서만 검색합니다.\n",
    "results = await db.amax_marginal_relevance_search(\"foo\", filter=dict(page=1))\n",
    "for doc in results:  # 검색 결과를 반복합니다.\n",
    "    # 각 문서의 내용과 메타데이터를 출력합니다.\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: foo, Metadata: {'page': 1}\n"
     ]
    }
   ],
   "source": [
    "# \"foo\"와 유사한 문서를 검색하고, 'page' 메타데이터가 1인 문서만 필터링하여 가장 유사한 1개의 문서를 반환하되, 4개의 문서를 가져옵니다.\n",
    "results = await db.asimilarity_search(\"foo\", filter=dict(page=1), k=1, fetch_k=4)\n",
    "for doc in results:  # 검색 결과를 반복하면서 각 문서에 대해 다음을 수행합니다.\n",
    "    # 문서의 내용과 메타데이터를 출력합니다.\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3a7aeb71-caad-4abc-811f-edfe59b7ac80'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index_to_docstore_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스 0에 해당하는 문서 저장소 ID를 사용하여 데이터베이스에서 문서를 삭제합니다.\n",
    "db.delete([db.index_to_docstore_id[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
